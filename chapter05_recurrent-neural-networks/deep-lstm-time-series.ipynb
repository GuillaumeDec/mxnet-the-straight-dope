{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Long Short Term Memory RNNs\n",
    "## For 1D time-series prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "from exceptions import ValueError\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "# we use cpus here\n",
    "ctx = mx.cpu(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='.*/IPython/.*')\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('poster')\n",
    "# Make inline plots vector graphics instead of raster graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# +1 because of the time shift between inputs and labels\n",
    "SEQ_LENGTH = 100 + 1\n",
    "NUM_SAMPLES_TRAINING = 5000 + 1\n",
    "# a few samples for testing our prediction outputs vs true labels\n",
    "NUM_SAMPLES_TESTING = 100 + 1\n",
    "# set to False if you already have the data files locally\n",
    "CREATE_DATA_SETS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: \"Some time-series\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return one random scalar from 0 to 1 using mxnet's nd\n",
    "def gimme_one_random_number():\n",
    "    return nd.random_uniform(low=0, high=1, shape=(1,1)).asnumpy()[0][0]\n",
    "\n",
    "# create one sine time series with fixed random frequency and amplitude, from Lakshmanan V. Medium's post\n",
    "def create_one_time_series(seq_length=10):\n",
    "  freq = (gimme_one_random_number()*0.5) + 0.1  # 0.1 to 0.6\n",
    "  ampl = gimme_one_random_number() + 0.5  # 0.5 to 1.5\n",
    "  x = np.sin(np.arange(0, seq_length) * freq) * ampl\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batch_time_series(seq_length=10, num_samples=4):\n",
    "    \"\"\"Create a dataframe with a batch of random sine time series. \n",
    "        inputs:\n",
    "            seq_length: number of time steps in each time series\n",
    "            num_samples: number of time series\n",
    "        outputs:\n",
    "            df: pandas dataframe of shape (num_samples, seq_length) with each row being one time series, \n",
    "            each column is a timestep\n",
    "    \"\"\"\n",
    "    column_labels = ['t'+str(i) for i in range(0, seq_length)]\n",
    "    df = pd.DataFrame(create_one_time_series(seq_length=seq_length)).transpose()\n",
    "    df.columns = column_labels\n",
    "    df.index = ['s'+str(0)]\n",
    "    for i in range(1, num_samples):\n",
    "        more_df = pd.DataFrame(create_one_time_series(seq_length=seq_length)).transpose()\n",
    "        more_df.columns = column_labels\n",
    "        more_df.index = ['s'+str(i)]\n",
    "        df = pd.concat([df, more_df], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Create some time-series for training and testing\n",
    "########################\n",
    "\n",
    "# build predictible random time-series\n",
    "mx.random.seed(123) \n",
    "\n",
    "# store the data in this directory, create it if it does not exist\n",
    "if not os.path.exists('../data/timeseries/'):\n",
    "    makedirs('../data/timeseries/')\n",
    "    \n",
    "if CREATE_DATA_SETS:\n",
    "    data_train = create_batch_time_series(seq_length=SEQ_LENGTH, num_samples=NUM_SAMPLES_TRAINING)  \n",
    "    data_test = create_batch_time_series(seq_length=SEQ_LENGTH, num_samples=NUM_SAMPLES_TESTING)\n",
    "    # Write data to csvs\n",
    "    data_train.to_csv(\"../data/timeseries/train.csv\")\n",
    "    data_test.to_csv(\"../data/timeseries/test.csv\")\n",
    "else: \n",
    "    data_train = pd.read_csv(\"../data/timeseries/train.csv\", index_col=0)\n",
    "    data_test = pd.read_csv(\"../data/timeseries/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pick 3 time series at random and plot them\n",
    "(data_train.sample(3).transpose().iloc[range(0, SEQ_LENGTH)]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of samples to use for each bach when doing minibatch training on the Deep net\n",
    "batch_size = 64\n",
    "# number of samples to use for testing. 1 is simple, to quickly evaluate prediction power on one time series\n",
    "batch_size_test = 1\n",
    "# sequence length for training & testing. This is equal to the number of RNN input (and output) cells when unrolling the RNN net. Can be adjusted to your liking or the problem you are trying to solve. Longer sequences may require deeper nets.\n",
    "seq_length = 16\n",
    "\n",
    "# number of minibatches available for training and testing\n",
    "num_batches_train = data_train.shape[0] // batch_size\n",
    "num_batches_test = data_test.shape[0] // batch_size_test\n",
    "\n",
    "#  we do 1D time series for now, this is equivalent to vocab_size = 1 for text \n",
    "num_features = 1  \n",
    "\n",
    "# inputs are from t0 to t_seq_length - 1. because the last point is kept for the output (\"labels\") of the penultimate point \n",
    "data_train_inputs = data_train.loc[:,data_train.columns[:-1]]\n",
    "data_train_labels = data_train.loc[:,data_train.columns[1:]]\n",
    "data_test_inputs = data_test.loc[:,data_test.columns[:-1]]\n",
    "data_test_labels = data_test.loc[:,data_test.columns[1:]]\n",
    "\n",
    "# reshape the data to prepare for training & testing ingestion\n",
    "train_data_inputs = nd.array(data_train_inputs.values).reshape((num_batches_train, batch_size, seq_length, num_features))\n",
    "train_data_labels = nd.array(data_train_labels.values).reshape((num_batches_train, batch_size, seq_length, num_features))\n",
    "test_data_inputs = nd.array(data_test_inputs.values).reshape((num_batches_test, batch_size_test, seq_length, num_features))\n",
    "test_data_labels = nd.array(data_test_labels.values).reshape((num_batches_test, batch_size_test, seq_length, num_features))\n",
    "\n",
    "train_data_inputs = nd.swapaxes(train_data_inputs, 1, 2)\n",
    "train_data_labels = nd.swapaxes(train_data_labels, 1, 2)\n",
    "test_data_inputs = nd.swapaxes(test_data_inputs, 1, 2)\n",
    "test_data_labels = nd.swapaxes(test_data_labels, 1, 2)\n",
    "\n",
    "print('num_samples_training={0} | num_batches_train={1} | batch_size={2} | seq_length={3}'.format(NUM_SAMPLES_TRAINING, num_batches_train, batch_size, seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short-term memory (LSTM) RNNs\n",
    "\n",
    "An LSTM block has mechanisms to enable \"memorizing\" information for an extended number of time steps. We use the LSTM block with the following transformations that map inputs to outputs across blocks at consecutive layers and consecutive time steps: $\\newcommand{\\xb}{\\mathbf{x}} \\newcommand{\\RR}{\\mathbb{R}}$\n",
    "\n",
    "$$g_t = \\text{tanh}(X_t W_{xg} + h_{t-1} W_{hg} + b_g),$$\n",
    "\n",
    "$$i_t = \\sigma(X_t W_{xi} + h_{t-1} W_{hi} + b_i),$$\n",
    "\n",
    "$$f_t = \\sigma(X_t W_{xf} + h_{t-1} W_{hf} + b_f),$$\n",
    "\n",
    "$$o_t = \\sigma(X_t W_{xo} + h_{t-1} W_{ho} + b_o),$$\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t,$$\n",
    "\n",
    "$$h_t = o_t \\odot \\text{tanh}(c_t),$$\n",
    "\n",
    "where $\\odot$ is an element-wise multiplication operator, and\n",
    "for all $\\xb = [x_1, x_2, \\ldots, x_k]^\\top \\in \\RR^k$ the two activation functions:\n",
    "\n",
    "$$\\sigma(\\xb) = \\left[\\frac{1}{1+\\exp(-x_1)}, \\ldots, \\frac{1}{1+\\exp(-x_k)}]\\right]^\\top,$$\n",
    "\n",
    "$$\\text{tanh}(\\xb) = \\left[\\frac{1-\\exp(-2x_1)}{1+\\exp(-2x_1)},  \\ldots, \\frac{1-\\exp(-2x_k)}{1+\\exp(-2x_k)}\\right]^\\top.$$\n",
    "\n",
    "In the transformations above, the memory cell $c_t$ stores the \"long-term\" memory in the vector form.\n",
    "In other words, the information accumulatively captured and  encoded  until time step $t$ is stored in $c_t$ and is only passed along the same layer over different time steps.\n",
    "\n",
    "Given the inputs $c_t$ and $h_t$, the input gate $i_t$ and forget gate $f_t$ will help the memory cell to decide how to overwrite or keep the memory information. The output gate $o_t$ further lets the LSTM block decide how to retrieve the memory information to generate the current state $h_t$ that is passed to both the next layer of the current time step and the next time step of the current layer. Such decisions are made using the hidden-layer parameters $W$ and $b$ with different subscripts: these parameters will be inferred during the training phase by ``gluon``.\n",
    "\n",
    "### Deep LSTM\n",
    "Deep LSTM are multi-layered, which means they contain at least two LSTM layers stacked together. One deep LSTM layer, indexed by $i$ (nameley, the $i$-th hidden LSTM layer, starting with $i=1$) receives the hidden state at time $t$, noted $h^{[i-1]}_t$, of the LSTM cell above it, and its own hidden state at time $t-1$, noted $h^{[i]}_{t-1}$ (that is, the hidden state of itself at the previous timestep, just like any regular one-layered LSTM RNN.)\n",
    "\n",
    "Like any regular one-layered LSTM, the entire sequence is fed to each LSTM unit, by design, and this is repeated for each sequence in the minibatch, before backprop and updating the parameters is done.\n",
    "\n",
    "Everything else is left unchanged. The output layer will receive inputs from the last hidden layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for a 1D time series, this is just a scalar equal to 1\n",
    "num_inputs = num_features\n",
    "# same comment\n",
    "num_outputs = num_features\n",
    "# num of hidden units in each hidden LSTM layer. This effectively sets the number of layers in the Deep Net.\n",
    "num_hidden_units = [8, 8, 8]\n",
    "# num of hidden LSTM layers\n",
    "num_hidden_layers = len(num_hidden_units)\n",
    "# num of units in each layer but the output layer\n",
    "num_units_layers = [num_features] + num_hidden_units\n",
    "\n",
    "########################\n",
    "#  Weights connecting the inputs to the hidden layers\n",
    "########################\n",
    "# weights and biases are now dictionaries because there is one set of them per layer\n",
    "Wxg, Wxi, Wxf, Wxo, Whg, Whi, Whf, Who, bg, bi, bf, bo = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {} \n",
    "for i_layer in range(1, num_hidden_layers+1):\n",
    "    num_inputs = num_units_layers[i_layer-1]\n",
    "    num_hidden_units = num_units_layers[i_layer]\n",
    "    Wxg[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "    Wxi[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "    Wxf[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "    Wxo[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "\n",
    "    ########################\n",
    "    #  Recurrent weights connecting the hidden layer across time steps\n",
    "    ########################\n",
    "    Whg[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "    Whi[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "    Whf[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "    Who[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "\n",
    "    ########################\n",
    "    #  Bias vector for hidden layer\n",
    "    ########################\n",
    "    bg[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "    bi[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "    bf[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "    bo[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "\n",
    "########################\n",
    "# Weights to the output nodes\n",
    "########################\n",
    "Why = nd.random_normal(shape=(num_units_layers[-1], num_outputs), ctx=ctx) * .01\n",
    "by = nd.random_normal(shape=num_outputs, ctx=ctx) * .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = []\n",
    "for i_layer in range(1, num_hidden_layers+1):\n",
    "    params += [Wxg[i_layer], Wxi[i_layer], Wxf[i_layer], Wxo[i_layer], Whg[i_layer], Whi[i_layer], Whf[i_layer], Who[i_layer], bg[i_layer], bi[i_layer], bf[i_layer], bo[i_layer]]\n",
    "\n",
    "# add the output layer\n",
    "params += [Why, by]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging the loss over the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we use standard rmse loss here, good enough for 1D time series\n",
    "def rmse(yhat, y):\n",
    "    return nd.mean(nd.sqrt(nd.sum(nd.power(y - yhat, 2), axis=0, exclude=True)))\n",
    "\n",
    "def average_rmse_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + rmse(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard Gradient Descent\n",
    "def SGD(params, learning_rate):\n",
    "    for param in params:\n",
    "        param[:] = param - learning_rate * param.grad\n",
    "\n",
    "# Adam Optimizer. Inspired from Agustinus Kristiadi's blog post on Optimizers\n",
    "def adam(params, learning_rate, M , R, index_adam_call, beta1, beta2, eps):\n",
    "    k = -1\n",
    "    for param in params:\n",
    "        k += 1\n",
    "        M[k] = beta1 * M[k] + (1. - beta1) * param.grad\n",
    "        R[k] = beta2 * R[k] + (1. - beta2) * (param.grad)**2\n",
    "        # bias correction since we initilized M & R to zeros, they're biased toward zero on the first few iterations\n",
    "        m_k_hat = M[k] / (1. - beta1**(index_adam_call))\n",
    "        r_k_hat = R[k] / (1. - beta2**(index_adam_call))\n",
    "        if((np.isnan(M[k].asnumpy())).any() or (np.isnan(R[k].asnumpy())).any()):\n",
    "            raise(ValueError('Error nans propagating in Adam Optimizer'))\n",
    "        param[:] = param - learning_rate * m_k_hat / (nd.sqrt(r_k_hat) + eps)\n",
    "    return params, M, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def single_lstm_unit_calcs(X, c, Wxg, h, Whg, bg, Wxi, Whi, bi, Wxf, Whf, bf, Wxo, Who, bo):\n",
    "    \"\"\" Function that does the LSTM computations for a single cell. \n",
    "        input:\n",
    "            all parameters (W, biases), input X, memory cell c.\n",
    "        output:\n",
    "            c: updated value for the memory cell. 'updated' means going from timestep t-1 to timestep t\n",
    "            h: updated value for the hidden state\n",
    "            \n",
    "    \"\"\"\n",
    "    g = nd.tanh(nd.dot(X, Wxg) + nd.dot(h, Whg) + bg)\n",
    "    i = nd.sigmoid(nd.dot(X, Wxi) + nd.dot(h, Whi) + bi)\n",
    "    f = nd.sigmoid(nd.dot(X, Wxf) + nd.dot(h, Whf) + bf)\n",
    "    o = nd.sigmoid(nd.dot(X, Wxo) + nd.dot(h, Who) + bo)\n",
    "    #######################\n",
    "    c = f * c + i * g\n",
    "    h = o * nd.tanh(c)\n",
    "    return c, h\n",
    "\n",
    "def deep_lstm_rnn(inputs, h, c, temperature=1.0):\n",
    "    \"\"\" Do one forward pass of the entire deep net (accross all layers) for one minibatch 'inputs'\n",
    "        h: dict of nd.arrays, each key is the index of a hidden layer (from 1 to num_hidden_layers). \n",
    "        Index 0, if any, is the input layer\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    # inputs is one MINIBATCH of sequences so its shape is number_of_seq, seq_length, features_dim \n",
    "    # Note that features_dim is 1 for a time series, vocab_size for a character, n for a n-D times series)\n",
    "    for X in inputs:  # this loop is therefore a loop on the timesteps\n",
    "        # X is one minibatch of one timestep value, NOT the entire sequence. E.g. if each batch has 37 sequences, then the first value of X will be a set of the 37 first values of each of the 37 sequences \n",
    "        # that means each iteration on X corresponds to one timestep value, but it is done in batches of different sequences\n",
    "        h[0] = X  # the first hidden layer takes the input X as input \n",
    "        for i_layer in range(1, num_hidden_layers+1):\n",
    "            # lstm units now have the 2 following inputs: \n",
    "            # i) h_t from the previous layer (equivalent to the input X for a non-deep lstm net), \n",
    "            # ii) h_t-1 from the current layer (same as for non-deep lstm nets)\n",
    "            c[i_layer], h[i_layer] = single_lstm_unit_calcs(h[i_layer-1], c[i_layer], Wxg[i_layer], h[i_layer], Whg[i_layer], bg[i_layer], Wxi[i_layer], Whi[i_layer], bi[i_layer], Wxf[i_layer], Whf[i_layer], bf[i_layer], Wxo[i_layer], Who[i_layer], bo[i_layer])\n",
    "        yhat_linear = nd.dot(h[num_hidden_layers], Why) + by\n",
    "        # yhat is a batch of several values of the same timestep\n",
    "        # this is basically the prediction of the next timestep value\n",
    "        # we cannot use a 1.0-bounded activation function like tanh for example, since amplitudes can be greater than 1.0\n",
    "        yhat = yhat_linear\n",
    "        # outputs has same shape as inputs, i.e. a list of minibatches of data points. outputs is thus a minibatch of output sequences\n",
    "        outputs.append(yhat) \n",
    "\n",
    "    return (outputs, h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_prediction(one_input_seq, one_label_seq, temperature=1.0):\n",
    "    #####################################\n",
    "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
    "    #####################################  # some better initialization needed??\n",
    "    h, c = {}, {}\n",
    "    for i_layer in range(1, num_hidden_layers+1):\n",
    "        h[i_layer] = nd.zeros(shape=(batch_size_test, num_units_layers[i_layer]), ctx=ctx)\n",
    "        c[i_layer] = nd.zeros(shape=(batch_size_test, num_units_layers[i_layer]), ctx=ctx)\n",
    "    \n",
    "    outputs, h, c = deep_lstm_rnn(one_input_seq, h, c, temperature=temperature)\n",
    "    loss = rmse(outputs[-1][0], one_label_seq)\n",
    "    return outputs[-1][0].asnumpy()[-1], one_label_seq.asnumpy()[-1], loss.asnumpy()[-1], outputs, one_label_seq\n",
    "\n",
    "def check_prediction(index):\n",
    "    \"\"\" Computes one prediction and its associated relative error on the index sequence of the test dataset\n",
    "        input:\n",
    "            index: index of the sequence in the test_data_inputs\n",
    "        output:\n",
    "            df: pandas dataframe that contains predicted values vs true values (called 'labels')\n",
    "    \"\"\"\n",
    "    o, label, loss, outputs, labels = test_prediction(test_data_inputs[index], test_data_labels[index], temperature=1.0)\n",
    "    prediction = round(o, 3)\n",
    "    true_label = round(label, 3)\n",
    "    outputs = [float(i.asnumpy().flatten()) for i in outputs]\n",
    "    true_labels = list(test_data_labels[index].asnumpy().flatten())\n",
    "    df = pd.DataFrame([outputs, true_labels]).transpose()\n",
    "    df.columns = ['predicted', 'true']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one epoch is one pass over the entire training dataset.\n",
    "epochs = 32\n",
    "moving_loss = 0.\n",
    "# 0.001 works for a [8, 8, 8] after about 70 epochs of 32-sized batches. \n",
    "# Can be somewhat larger for deeper nets or when using SGD, to speed up training.\n",
    "learning_rate = 0.001  # \n",
    "\n",
    "# Adam Optimizer parameters\n",
    "beta1 = .9\n",
    "beta2 = .999\n",
    "index_adam_call = 0\n",
    "# M & R arrays to keep track of momenta in adam optimizer. params is a list that contains all ndarrays of parameters\n",
    "M = {k: nd.zeros_like(v) for k, v in enumerate(params)}\n",
    "R = {k: nd.zeros_like(v) for k, v in enumerate(params)}\n",
    "\n",
    "# initialize dataframes that keep track of Loss and Errors of test predictions, for visualization purposes\n",
    "df_moving_loss = pd.DataFrame(columns=['Loss', 'Error'])\n",
    "df_moving_loss.index.name = 'Epoch'\n",
    "\n",
    "# needed to update plots on the fly\n",
    "%matplotlib notebook\n",
    "fig, axes_fig1 = plt.subplots(1,1, figsize=(6,3))  # for plotting predicted vs true time series\n",
    "fig2, axes_fig2 = plt.subplots(1,1, figsize=(6,3))  # for plotting training Loss and Relative error on predictions for a randomly chosen test sequence\n",
    "\n",
    "for e in range(epochs):\n",
    "    ############################\n",
    "    # Attenuate the learning rate by a factor of 2 every 100 epochs. Only use for SGD, not needed (valid?) for Adam.\n",
    "    ############################\n",
    "    if ((e+1) % 100 == 0):\n",
    "        learning_rate = learning_rate / 2.0\n",
    "\n",
    "    # initialize hidden and memory states for the net \n",
    "    h, c = {}, {}\n",
    "    for i_layer in range(1, num_hidden_layers+1):\n",
    "        h[i_layer] = nd.zeros(shape=(batch_size, num_units_layers[i_layer]), ctx=ctx)\n",
    "        c[i_layer] = nd.zeros(shape=(batch_size, num_units_layers[i_layer]), ctx=ctx)\n",
    "\n",
    "    for i in range(num_batches_train):\n",
    "        data_one_hot = train_data_inputs[i]\n",
    "        label_one_hot = train_data_labels[i]\n",
    "        with autograd.record():\n",
    "            outputs, h, c = deep_lstm_rnn(data_one_hot, h, c)\n",
    "            loss = average_rmse_loss(outputs, label_one_hot)\n",
    "            loss.backward()\n",
    "#         SGD(params, learning_rate)\n",
    "        index_adam_call += 1  # needed for bias correction in Adam optimizer\n",
    "        params, M, R = adam(params, learning_rate, M, R, index_adam_call, beta1, beta2, 1e-8)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if (i == 0) and (e == 0):\n",
    "            moving_loss = nd.mean(loss).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
    "        df_moving_loss.loc[e] = round(moving_loss, 4)\n",
    "\n",
    "    ############################\n",
    "    #  Predictions and plots\n",
    "    ############################\n",
    "    # use the epoch index to randomly select one test data sequence.\n",
    "    data_prediction_df = check_prediction(index=e)\n",
    "    axes_fig1.clear()\n",
    "    data_prediction_df.plot(ax=axes_fig1)\n",
    "    fig.canvas.draw()\n",
    "    prediction = round(data_prediction_df.tail(1)['predicted'].values.flatten()[-1], 3)\n",
    "    true_label = round(data_prediction_df.tail(1)['true'].values.flatten()[-1], 3)\n",
    "    rel_error = round(100. * np.abs(prediction / true_label - 1.0), 2)\n",
    "    axes_fig2.clear()\n",
    "    if e == 0:\n",
    "        moving_rel_error = rel_error\n",
    "    else:\n",
    "        moving_rel_error = .9 * moving_rel_error + .1 * rel_error\n",
    "\n",
    "    df_moving_loss.loc[e, ['Error']] = moving_rel_error\n",
    "    axes_loss_plot = df_moving_loss.plot(ax=axes_fig2, secondary_y='Loss', color=['r','b'])\n",
    "    axes_loss_plot.right_ax.grid(False)\n",
    "    axes_loss_plot.right_ax.set_yscale('log')\n",
    "    fig2.canvas.draw()\n",
    "    print(\"Epoch = {0} | Loss = {1} | Out of Sample Prediction = {2} True = {3} Error = {4}\".format(e, moving_loss, prediction, true_label, moving_rel_error ))\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "After about 10 epochs, the deep net already exhibits a descent prediction power.\n",
    "\n",
    "After about 30 epochs, convergence slows down and prediction power is of the order of a few percents.\n",
    "At some point, the deep net may diverge and NaNs may appear.\n",
    "\n",
    "The Deep Net can be customized by changing it shape using num_hidden_units. A deeper will require more training data, a longer training, without bringing too much extra performance. This is because the underlying data is just sine waves so their complexity is limited. A super-complex (i.e. deep with many hidden cells) net that tries to learn a not-so-complex time series is a sure path to unnecessary slow training at best and to overfitting at worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
